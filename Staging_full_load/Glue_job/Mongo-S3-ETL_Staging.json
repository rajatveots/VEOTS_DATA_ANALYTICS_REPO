{
	"jobConfig": {
		"name": "Mongo-S3-ETL_Staging",
		"description": "",
		"role": "arn:aws:iam::747036646062:role/AWSGlueServiceRole-Mongodb",
		"command": "glueetl",
		"version": "3.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": null,
		"maxRetries": 0,
		"timeout": 50,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "Mongo-S3-ETL_Staging.py",
		"scriptLocation": "s3://aws-glue-assets-747036646062-ap-south-1/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--additional-python-modules",
				"value": "pymongo",
				"existing": false
			},
			{
				"key": "--bucket",
				"value": "sf-stage-veots",
				"existing": false
			},
			{
				"key": "--db",
				"value": "staging-veots",
				"existing": false
			},
			{
				"key": "--excluded_clients",
				"value": "tes7048,has6553",
				"existing": false
			},
			{
				"key": "--mongo_uri",
				"value": "mongodb+srv://data_engineer:TrrVDS61ebxZl2UO@veots-cluster0.urbh6.mongodb.net/?retryWrites=true&w=majority",
				"existing": false
			}
		],
		"tags": [
			{
				"key": "Name",
				"value": "ETL",
				"existing": false
			}
		],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2024-05-03T06:57:15.824Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-747036646062-ap-south-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-747036646062-ap-south-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "##Created By Mudit For loading MongoDB collections Data into S3 in Parquet Format\r\n\r\n\r\nimport sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pymongo import MongoClient\r\nfrom pyspark.sql.functions import udf\r\nfrom pyspark.sql.types import *\r\nfrom pyspark.sql.functions import lit\r\nfrom pyspark.sql.functions import col\t\r\n\r\n\r\nimport time\r\nfrom datetime import date\r\nimport boto3\r\nfrom pymongo.mongo_client import MongoClient\r\nfrom pymongo.server_api import ServerApi\r\n\r\n  \r\nsc = SparkContext.getOrCreate()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\n\r\nargs = getResolvedOptions(sys.argv,['db','bucket','mongo_uri','excluded_clients'])\r\ndb_name =args['db']\r\nbucket_name=args['bucket']\r\nmongo_uri =args['mongo_uri']\r\nexcluded_clients=args['excluded_clients']\r\n\r\ndef get_all_collections(db):\r\n   #Get names of all the collection in MongoDB Database\r\n    uri = mongo_uri\r\n    client = MongoClient(uri, server_api=ServerApi('1'))\r\n    mydatabase = client[db]\r\n    collections = mydatabase.list_collection_names()\r\n\r\n    return collections\r\n\r\ndef collection_lists(collections):\r\n    #Create list of all the static and Dynamic Collections\r\n    tors = []\r\n    dors = []\r\n    vectortable4_1 =[]\r\n    vectortable4_2 =[]\r\n    vectortable6=[]\r\n    vectortable7=[]\r\n    vectortable8=[]\r\n    vectortable9=[]\r\n    static_coll =[]\r\n    \r\n    for i in collections :\r\n   \r\n        if '_tors' in i :\r\n            tors.append(i)\r\n        if '_dors' in i :\r\n            dors.append(i)\r\n        if '_vectortable4_1' in i :\r\n            vectortable4_1.append(i)\r\n        if '_vectortable4_2' in i :\r\n            vectortable4_2.append(i)    \r\n        if '_vectortable6' in i :\r\n            vectortable6.append(i)\r\n        if '_vectortable7' in i :\r\n            vectortable7.append(i) \r\n        if '_vectortable8' in i :\r\n            vectortable8.append(i) \r\n        if '_vectortable9' in i :\r\n            vectortable9.append(i)    \r\n            \r\n\r\n        \r\n    static_coll = list (set(collections) -set(tors) -set(dors) - set(vectortable4_1) - set(vectortable4_2) -set(vectortable6) -set(vectortable7)-set(vectortable8)-set(vectortable9))        \r\n    return tors,dors,vectortable4_1,vectortable4_2,static_coll , vectortable6,vectortable7,vectortable8,vectortable9\r\n\r\ndef none_type_to_str(df):\r\n# get dataframe schema\r\n    my_schema = list(df.schema)\r\n    \r\n    null_cols = []\r\n    \r\n    # iterate over schema list to filter for NullType columns\r\n    for st in my_schema:\r\n        if str(st.dataType) == 'NullType':\r\n            null_cols.append(st)\r\n    \r\n    # cast null type columns to string (or whatever you'd like)\r\n    for ncol in null_cols:\r\n        mycolname = str(ncol.name)\r\n        df = df \\\r\n            .withColumn(mycolname, df[mycolname].cast('string'))\r\n    return df\r\n    # import the time module\r\n\r\ndef mongo_to_DF(collection_name,DB_Name,mongo_uri):\r\n    #Returns A MongoDB collection in a DF\r\n  \r\n    try:\r\n       df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\r\n       .option(\"uri\", mongo_uri)\\\r\n       .option(\"spark.mongodb.input.database\",DB_Name)\\\r\n       .option(\"spark.mongodb.input.collection\",collection_name)\\\r\n       .load\r\n       \r\n       #df = none_type_to_str(df)\r\n       \r\n       return df \r\n\r\n    except Exception as e:        \r\n       return (str(e) + ' in collection -->'  + collection_name )\r\n        \r\n\r\ndef write_to_s3_storage(df,bucket_name,folder_name,file_name,coll_name,dt):\r\n    #write to S3 Storage\r\n    s3 = boto3.client('s3')\r\n    s3_url = 's3://' + bucket_name + '/' +folder_name + '/' + coll_name + '/' +dt + '/'\r\n    print(s3_url)\r\n    s3.put_object(Bucket=bucket_name , Key=(folder_name + '/'))\r\n    df.write.json(s3_url + file_name)\r\n\r\ndef delete_from_S3(bucket,folder_location):\r\n    folder = folder_location +'/'\r\n    s3 = boto3.resource('s3')\r\n    try:\r\n        bucket = s3.Bucket(bucket)\r\n        bucket.objects.filter(Prefix=folder).delete()\r\n        return True\r\n    except Exception as e:\r\n        print(f\"Failed to delete s3 folder : {e}\")\r\n        return False    \r\n\r\ndef save_collections_to_S3(coll_list,folder_name,db,bucket_name,mongo_uri):\r\n    #save Mongo collection to S3 . Parameters --> list of collection, Folder Name\r\n    \r\n    import time\r\n    from datetime import date\r\n\r\n    dt = date.today()\r\n\r\n# get the current time in seconds since the epoch\r\n  \r\n    seconds = time.time()\r\n    try:\r\n        for i in coll_list:\r\n            try:\r\n                df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\r\n                .option(\"uri\", mongo_uri)\\\r\n                .option(\"spark.mongodb.input.database\",db)\\\r\n                .option(\"spark.mongodb.input.collection\",i).option(\"sampleSize\", 10000).load()\r\n                \r\n                \r\n                #df = df.fillna(\"\")\r\n\r\n                \r\n                bucket_name = bucket_name\r\n                #folder_name = db + '/' + folder_name\r\n                coll_name =i\r\n                file_name = i + '_' + str(int( time.time())) +'.json'\r\n                print(file_name)\r\n                #df.show()\r\n                if df.count() > 0 :\r\n                    write_to_s3_storage(df,bucket_name,db +'/'+ folder_name,file_name,coll_name,str(dt))\r\n                else:\r\n                    print(\"collection has 0 records: \" + coll_name)\r\n            except Exception as e:        \r\n                print(str(e) + ' in collection' + i)\r\n                continue\r\n        \r\n    except Exception as e:\r\n        print(str(e) + ' in collection' + i)\r\n\r\n\r\ndef main():\r\n    db = db_name\r\n    testingdb = get_all_collections(db)\r\n    #coupons = get_all_collections(\"coupons\")\r\n    \r\n    exclude_client_list=excluded_clients.split(',')\r\n    \r\n    # Create a new list excluding the unwanted collections\r\n    filtered_coll_list = []\r\n    filtered_coll_list = [j for j in testingdb if not any(i in j for i in exclude_client_list)]\r\n\r\n    tors,dors,vectortable4_1,vectortable4_2,static_coll , vectortable6,vectortable7,vectortable8,vectortable9 = collection_lists(filtered_coll_list)\r\n    static_coll\r\n    coll_folders =['tors','dors','vectortable4_1','vectortable4_2','static_coll' , 'vectortable6','vectortable7','vectortable8','vectortable9']\r\n    coll_list=[tors,dors,vectortable4_1,vectortable4_2,static_coll , vectortable6,vectortable7,vectortable8,vectortable9]\r\n    \r\n    i =0 \r\n    bucket = bucket_name\r\n    folder_location = db\r\n    delete_from_S3(bucket,folder_location)\r\n    while i<len(coll_folders):\r\n        save_collections_to_S3(coll_list[i],coll_folders[i],folder_location,bucket,mongo_uri)\r\n        i =i+1\r\n    #db = coupons_db\r\n    #coupons = get_all_collections(db)\r\n    #bucket = bucket_name\r\n    #folder_location = db\r\n    #delete_from_S3(bucket,folder_location)\r\n    #save_collections_to_S3(coupons,db,folder_location,bucket,mongo_uri)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n    \r\njob.commit()"
}